{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1faca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8701cb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 23:51:52 INFO mlflow.tracking.fluent: Experiment with name 'evaluation_with_prompt_registry' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from utils import get_random_files, get_image, get_json, _set_openai_api_key_for_demo\n",
    "\n",
    "# Set MLflow tracking URI to cwd()\n",
    "mlflow.set_tracking_uri(os.getcwd() + \"/mlruns\")\n",
    "mlflow.set_experiment(\"evaluation_with_prompt_registry\")\n",
    "\n",
    "# Get API key securely\n",
    "if (not _set_openai_api_key_for_demo()) and (not os.getenv(\"OPENAI_API_KEY\")):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ed5dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_files = get_random_files(n=5)\n",
    "images = [get_image(file, encode_as_str=True) for file in _files]\n",
    "jsons = [get_json(file) for file in _files]\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d496af",
   "metadata": {},
   "source": [
    "## Prompt Engineer: Improve the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ad9ede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(name=ocr-question-answer, version=1, template=You are an expert at key infor...)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "new_template = \"\"\"\\\n",
    "You are an expert at key information extraction and OCR.\n",
    "\n",
    "Format as a list of dictionaries as shown below. They keys should only be `question` and `answer`. \n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"question\": \"question field\",\n",
    "        \"answer\": \"answer to question field\"\n",
    "\n",
    "    },\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "Question refers to a field in the form that takes in information. Answer refers to the information \n",
    "that is filled in the field.\n",
    "\n",
    "Follow these rules:\n",
    "- Only use the information present in the text.\n",
    "{{ additional_rules }}\n",
    "\"\"\"\n",
    "\n",
    "# Register a new version of an existing prompt\n",
    "updated_prompt = mlflow.register_prompt(\n",
    "    name=\"ocr-question-answer\",\n",
    "    template=new_template,\n",
    "    version_metadata={\n",
    "        \"author\": \"author@example.com\",\n",
    "    },\n",
    ")\n",
    "\n",
    "updated_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8413618",
   "metadata": {},
   "source": [
    "## ML Engineer: Use the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b192dcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(name=ocr-question-answer, version=1, template=You are an expert at key infor...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = mlflow.load_prompt(\"prompts:/ocr-question-answer/latest\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10fddde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"question\": \"CASE NAME\",\n",
      "        \"answer\": \"Donald D. Sellers and Robin J. Sellers v. Raybestos-Manhattan, et al.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"COURT\",\n",
      "        \"answer\": \"San Francisco Superior Court - No. 996382\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"LORILLARD ENTITIES\",\n",
      "        \"answer\": \"Lorillard Tobacco Company\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"DATE FILED\",\n",
      "        \"answer\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"DATE SERVED\",\n",
      "        \"answer\": \"August 3, 1998\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"CASE TYPE\",\n",
      "        \"answer\": \"Asbestos\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"PLAINTIFF'S COUNSEL\",\n",
      "        \"answer\": \"Wartnick, Chaber, Harowitz, Smith & Tigerman\\nStephen M. Tigerman\\n101 California Street, Suite 2200\\nSan Francisco, California 94111\\n415/986-5566\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"LORILLARD COUNSEL\",\n",
      "        \"answer\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"JUDGE\",\n",
      "        \"answer\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"TRIAL DATE\",\n",
      "        \"answer\": \"\"\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def get_completion(inputs: str) -> str:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": prompt.format( # Add system prompt here\n",
    "                    additional_rules=\"Use exact formatting you see in the form.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"type\": \"text\", \"text\": \"what's in this image?\" },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{inputs}\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    predicted = get_completion(images[0])\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3b5bc",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b16af9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 23:53:28 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025/05/12 23:54:09 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/05/12 23:54:09 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/05/12 23:54:09 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/05/12 23:54:09 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/05/12 23:54:09 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/05/12 23:54:09 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/05/12 23:54:09 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "2025/05/12 23:54:11 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/05/12 23:54:11 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/05/12 23:54:11 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/05/12 23:54:11 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/05/12 23:54:11 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/05/12 23:54:11 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_format = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"correct_format\",\n",
    "    definition=(\n",
    "        \"\"\"The answer is a list of dicts where keys are `question` and `answer`.\"\"\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"\"\"If formatted correctly, return 1. Otherwise, return 0.\"\"\"\n",
    "    ),\n",
    "    model=\"openai:/gpt-4o-mini\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "def batch_completion(df: pd.DataFrame) -> list[str]:\n",
    "    return [get_completion(image) for image in df[\"inputs\"]]\n",
    "\n",
    "eval_result = mlflow.evaluate(\n",
    "    model=batch_completion,\n",
    "    data=pd.DataFrame({\"inputs\": images, \"truth\": jsons}),\n",
    "    targets=\"truth\",\n",
    "    model_type=\"text\",\n",
    "    extra_metrics=[correct_format],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d894ffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 585.14it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 558.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>correct_format/v1/score</th>\n",
       "      <th>correct_format/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'COURT:': 'San Francisco Superior Court- No. ...</td>\n",
       "      <td>The image is a case form document. Here are th...</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is correctly formatted as a list of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'': 'J. D. Ergle and R. F. Dufresne', 'DATE':...</td>\n",
       "      <td>The image contains a form titled \"DECISION TRE...</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is correctly formatted as a list of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'DATE:': '8/ 10/ 90', 'MANUFACTURER:': 'B &amp; W...</td>\n",
       "      <td>```json\\n[\\n    {\\n        \"question\": \"REPORT...</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is correctly formatted as a list of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'BRAND': 'STYLE SLIM MEN. LT. 100's', 'NOTE:'...</td>\n",
       "      <td>The image is a \"Direct Account Status Report\" ...</td>\n",
       "      <td>509</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is correctly formatted as a list of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'TO:': 'George Baroody', 'DATE:': '12 /10 /98...</td>\n",
       "      <td>```json\\n[\\n    {\\n        \"question\": \"TO\",\\n...</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is correctly formatted as a list of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "1  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "2  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "3  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "4  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "\n",
       "                                               truth  \\\n",
       "0  {'COURT:': 'San Francisco Superior Court- No. ...   \n",
       "1  {'': 'J. D. Ergle and R. F. Dufresne', 'DATE':...   \n",
       "2  {'DATE:': '8/ 10/ 90', 'MANUFACTURER:': 'B & W...   \n",
       "3  {'BRAND': 'STYLE SLIM MEN. LT. 100's', 'NOTE:'...   \n",
       "4  {'TO:': 'George Baroody', 'DATE:': '12 /10 /98...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  The image is a case form document. Here are th...          302   \n",
       "1  The image contains a form titled \"DECISION TRE...          193   \n",
       "2  ```json\\n[\\n    {\\n        \"question\": \"REPORT...          268   \n",
       "3  The image is a \"Direct Account Status Report\" ...          509   \n",
       "4  ```json\\n[\\n    {\\n        \"question\": \"TO\",\\n...          178   \n",
       "\n",
       "   correct_format/v1/score                    correct_format/v1/justification  \n",
       "0                        1  The output is correctly formatted as a list of...  \n",
       "1                        1  The output is correctly formatted as a list of...  \n",
       "2                        1  The output is correctly formatted as a list of...  \n",
       "3                        1  The output is correctly formatted as a list of...  \n",
       "4                        1  The output is correctly formatted as a list of...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.tables['eval_results_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85c29c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correct_format/v1/mean': np.float64(1.0), 'correct_format/v1/variance': np.float64(0.0), 'correct_format/v1/p90': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "print(eval_result.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odsc-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

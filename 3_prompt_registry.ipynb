{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1faca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8701cb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 23:51:52 INFO mlflow.tracking.fluent: Experiment with name 'evaluation_with_prompt_registry' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from utils import get_random_files, get_image, get_json, _set_openai_api_key_for_demo\n",
    "\n",
    "# Set MLflow tracking URI to cwd()\n",
    "mlflow.set_tracking_uri(os.getcwd() + \"/mlruns\")\n",
    "mlflow.set_experiment(\"evaluation_with_prompt_registry\")\n",
    "\n",
    "# Get API key securely\n",
    "if (not _set_openai_api_key_for_demo()) and (not os.getenv(\"OPENAI_API_KEY\")):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ed5dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_files = get_random_files(n=5)\n",
    "images = [get_image(file, encode_as_str=True) for file in _files]\n",
    "jsons = [get_json(file) for file in _files]\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d496af",
   "metadata": {},
   "source": [
    "## Prompt Engineer: Improve the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ad9ede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(name=ocr-question-answer, version=1, template=You are an expert at key infor...)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "new_template = \"\"\"\\\n",
    "You are an expert at key information extraction and OCR.\n",
    "\n",
    "Format as a list of dictionaries as shown below. They keys should only be `question` and `answer`. \n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"question\": \"question field\",\n",
    "        \"answer\": \"answer to question field\"\n",
    "\n",
    "    },\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "Question refers to a field in the form that takes in information. Answer refers to the information \n",
    "that is filled in the field.\n",
    "\n",
    "Follow these rules:\n",
    "- Only use the information present in the text.\n",
    "{{ additional_rules }}\n",
    "\"\"\"\n",
    "\n",
    "# Register a new version of an existing prompt\n",
    "updated_prompt = mlflow.register_prompt(\n",
    "    name=\"ocr-question-answer\",\n",
    "    template=new_template,\n",
    "    version_metadata={\n",
    "        \"author\": \"author@example.com\",\n",
    "    },\n",
    ")\n",
    "\n",
    "updated_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8413618",
   "metadata": {},
   "source": [
    "## ML Engineer: Use the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b192dcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(name=ocr-question-answer, version=1, template=You are an expert at key infor...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = mlflow.load_prompt(\"prompts:/ocr-question-answer/latest\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10fddde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"question\": \"CASE NAME\",\n",
      "        \"answer\": \"Donald D. Sellers and Robin J. Sellers v. Raybestos-Manhattan, et al.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"COURT\",\n",
      "        \"answer\": \"San Francisco Superior Court - No. 996382\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"LORILLARD ENTITIES\",\n",
      "        \"answer\": \"Lorillard Tobacco Company\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"DATE FILED\",\n",
      "        \"answer\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"DATE SERVED\",\n",
      "        \"answer\": \"August 3, 1998\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"CASE TYPE\",\n",
      "        \"answer\": \"Asbestos\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"PLAINTIFF'S COUNSEL\",\n",
      "        \"answer\": \"Wartnick, Chaber, Harowitz, Smith & Tigerman\\nStephen M. Tigerman\\n101 California Street, Suite 2200\\nSan Francisco, California 94111\\n415/986-5566\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"LORILLARD COUNSEL\",\n",
      "        \"answer\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"JUDGE\",\n",
      "        \"answer\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"TRIAL DATE\",\n",
      "        \"answer\": \"\"\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def get_completion(inputs: str) -> str:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": prompt.format( # Add system prompt here\n",
    "                    additional_rules=\"Use exact formatting you see in the form.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"type\": \"text\", \"text\": \"what's in this image?\" },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{inputs}\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    predicted = get_completion(images[0])\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3b5bc",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16af9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 23:29:07 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "2025/05/12 23:29:07 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025/05/12 23:29:55 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/05/12 23:29:57 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/05/12 23:29:57 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/05/12 23:29:57 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/05/12 23:29:57 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/05/12 23:29:57 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/05/12 23:29:57 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "/Users/michael.berk/opt/anaconda3/envs/odsc-3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "2025/05/12 23:29:59 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/05/12 23:29:59 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/05/12 23:29:59 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/05/12 23:29:59 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/05/12 23:29:59 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/05/12 23:29:59 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_format = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"correct_format\",\n",
    "    definition=(\n",
    "        \"\"\"The answer is a list of dicts where keys are `question` and `answer`.\"\"\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"\"\"If formatted correctly, return 1. Otherwise, return 0.\"\"\"\n",
    "    ),\n",
    "    model=\"openai:/gpt-4o-mini\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "def batch_completion(df: pd.DataFrame) -> list[str]:\n",
    "    return [get_completion(image) for image in df[\"inputs\"]]\n",
    "\n",
    "eval_result = mlflow.evaluate(\n",
    "    model=batch_completion,\n",
    "    data=pd.DataFrame({\"inputs\": images, \"truth\": jsons}),\n",
    "    targets=\"truth\",\n",
    "    model_type=\"text\",\n",
    "    extra_metrics=[correct_format],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d894ffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 823.87it/s] \n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 1007.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>correct_format/v1/score</th>\n",
       "      <th>correct_format/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'To:': 'Mike Mozina', 'Firm:': 'MSA', 'FAX': ...</td>\n",
       "      <td>[\\n    {\\n        \"question\": \"To\",\\n        \"...</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is correctly formatted as a list of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'DATE:': '8/ 10/ 90', 'MANUFACTURER:': 'B &amp; W...</td>\n",
       "      <td>```json\\n[\\n    {\\n        \"question\": \"REPORT...</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is correctly formatted as a list of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'TO:': 'K. A. Sparrow', 'FROM:': 'T. D. Blach...</td>\n",
       "      <td>The image is a document titled \"OLD GOLD MENTH...</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>The output does not match the required format ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'cc:': 'D. O. S.', ':': 'R. W. Caldarella', '...</td>\n",
       "      <td>```json\\n[\\n    {\\n        \"question\": \"Submis...</td>\n",
       "      <td>401</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is correctly formatted as a list of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...</td>\n",
       "      <td>{'COURT:': 'San Francisco Superior Court- No. ...</td>\n",
       "      <td>```json\\n[\\n    {\\n        \"question\": \"CASE N...</td>\n",
       "      <td>287</td>\n",
       "      <td>1</td>\n",
       "      <td>The output is correctly formatted as a list of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "1  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "2  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "3  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "4  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDABQODxIPDRQSEB...   \n",
       "\n",
       "                                               truth  \\\n",
       "0  {'To:': 'Mike Mozina', 'Firm:': 'MSA', 'FAX': ...   \n",
       "1  {'DATE:': '8/ 10/ 90', 'MANUFACTURER:': 'B & W...   \n",
       "2  {'TO:': 'K. A. Sparrow', 'FROM:': 'T. D. Blach...   \n",
       "3  {'cc:': 'D. O. S.', ':': 'R. W. Caldarella', '...   \n",
       "4  {'COURT:': 'San Francisco Superior Court- No. ...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  [\\n    {\\n        \"question\": \"To\",\\n        \"...          228   \n",
       "1  ```json\\n[\\n    {\\n        \"question\": \"REPORT...          268   \n",
       "2  The image is a document titled \"OLD GOLD MENTH...          288   \n",
       "3  ```json\\n[\\n    {\\n        \"question\": \"Submis...          401   \n",
       "4  ```json\\n[\\n    {\\n        \"question\": \"CASE N...          287   \n",
       "\n",
       "   correct_format/v1/score                    correct_format/v1/justification  \n",
       "0                        1  The output is correctly formatted as a list of...  \n",
       "1                        1  The output is correctly formatted as a list of...  \n",
       "2                        0  The output does not match the required format ...  \n",
       "3                        1  The output is correctly formatted as a list of...  \n",
       "4                        1  The output is correctly formatted as a list of...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.tables['eval_results_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85c29c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correct_format/v1/mean': np.float64(0.8), 'correct_format/v1/variance': np.float64(0.16), 'correct_format/v1/p90': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "print(eval_result.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odsc-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
